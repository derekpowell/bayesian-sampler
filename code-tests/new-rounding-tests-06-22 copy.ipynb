{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "american-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "numpyro.enable_x64()\n",
    "numpyro.util.set_host_device_count(4)\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from numpyro.infer import DiscreteHMCGibbs, MCMC, NUTS, Predictive, SA\n",
    "from numpyro.contrib.funsor import config_enumerate\n",
    "\n",
    "from jax.scipy.special import digamma, polygamma, logit, expit, erf, gammaln\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import pandas as pd\n",
    "import siuba as s\n",
    "\n",
    "from numpy import exp, log\n",
    "\n",
    "import scipy\n",
    "\n",
    "from plotnine import *\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "valued-current",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.7500001 , 0.91629994], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from numerical recipes, licensing unclear\n",
    "# http://www.it.uom.gr/teaching/linearalgebra/NumericalRecipiesInC/c6-4.pdf\n",
    "\n",
    "from jax import grad, jit\n",
    "\n",
    "# @jit # can't jit this but probabl doesn't matter?\n",
    "def betaai_lax(a,b,x):\n",
    "    # need to modify this to handle multidimensional inputs! maybe using vmap\n",
    "    \n",
    "    bt = jnp.where(\n",
    "        jnp.logical_or(x==0, x==1),\n",
    "        0.,\n",
    "        jnp.exp(gammaln(a+b) - gammaln(a) - gammaln(b) + a*jnp.log(x) + b*jnp.log(1.-x))\n",
    "    )\n",
    "    \n",
    "    res = jnp.where(\n",
    "        x < (a+1.0)/(a+b+2.),\n",
    "        bt*vec_betacf_lax(a,b,x)/a,\n",
    "        1. - bt*vec_betacf_lax(b,a,1.-x)/b\n",
    "    )\n",
    "    \n",
    "    divisor = jnp.where(jnp.logical_or(x <0.0, x > 1.0), 0., 1.) # create NAN if bad inputs\n",
    "    \n",
    "    return res/divisor\n",
    "\n",
    "@jit\n",
    "def lentz_method(m, carry):\n",
    "    \n",
    "    a = carry[0]\n",
    "    b = carry[1]\n",
    "    c = carry[2]\n",
    "    d = carry[3]\n",
    "    h = carry[4]\n",
    "    x = carry[5]\n",
    "\n",
    "    FPMIN = 1e-30\n",
    "    \n",
    "    qab = a+b\n",
    "    qap = a + 1.\n",
    "    qam = a-1. \n",
    "    \n",
    "    \n",
    "    numerator = m*(b-m)*x/((qam+m*2.) * (a+m*2.) )  \n",
    "        \n",
    "    # odd step\n",
    "    d = 1. + numerator*d # depends on last step\n",
    "    d = jnp.clip(d,FPMIN, jnp.inf)\n",
    "    d = 1./d\n",
    "\n",
    "    c = jnp.clip(c,FPMIN, jnp.inf)\n",
    "    c = 1.0+numerator/c\n",
    "\n",
    "    h = h*d*c\n",
    "\n",
    "    # even step\n",
    "    numerator = -(a+m) * (qab+m)*x/( (a+m*2.)*(qap+m*2.))\n",
    "\n",
    "    d = 1. + numerator*d \n",
    "    d = jnp.clip(d,FPMIN, jnp.inf)\n",
    "    d = 1./d\n",
    "\n",
    "    c = jnp.clip(c,FPMIN, jnp.inf)\n",
    "    c = 1.+numerator/c\n",
    "\n",
    "    h = h*d*c\n",
    "    \n",
    "    return (a,b,c,d,h,x)\n",
    "\n",
    "vec_lentz = jnp.vectorize(lentz_method)\n",
    "\n",
    "@jit\n",
    "def betacf_lax(a,b,x):\n",
    "\n",
    "    FPMIN = 1e-30\n",
    "    \n",
    "    qab = a+b\n",
    "    qap = a + 1.\n",
    "    qam = a-1. \n",
    "\n",
    "    # initialize\n",
    "    c = 1.0\n",
    "    d = 1.0 - qab*x/qap\n",
    "    d = jnp.clip(d,FPMIN, jnp.inf)\n",
    "    d = 1./d\n",
    "    h=d \n",
    "\n",
    "    res = jax.lax.fori_loop(1, 101, lentz_method, init_val = (a,b,c,d,h, x))\n",
    "    \n",
    "    return(res[4])\n",
    "    \n",
    "    \n",
    "vec_betacf_lax = jnp.vectorize(betacf_lax)\n",
    "\n",
    "betaai_lax(jnp.array([1,2]), jnp.array([2,3]), jnp.array([.5, .7]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "quick-method",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(nan, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note need newer Jax to get autoddiff\n",
    "# but still doesn't work in multidimensional case w/ grad itself\n",
    "jax.grad(betaai_lax)(3., 3., .9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-receiver",
   "metadata": {},
   "source": [
    "Looks like this is unlikely to work as the gradients calculated are very unstable, often returning nan\n",
    "\n",
    "## Implementing the gradients with `custom_jvp`\n",
    "\n",
    "The defjvps convenience wrapper lets us define a JVP for each argument separately, and the results are computed separately then summed:\n",
    "\n",
    "```\n",
    "@custom_jvp\n",
    "def f(x, y):\n",
    "  return x ** 2 * y\n",
    "\n",
    "f.defjvps(lambda x_dot, primal_out, x, y: 2 * x * y * x_dot,\n",
    "          lambda y_dot, primal_out, x, y: x ** 2 * y_dot)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "beautiful-import",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.34642628\n",
      "0.34642622\n",
      "1.1999998\n"
     ]
    }
   ],
   "source": [
    "from jax import custom_jvp\n",
    "from jax.scipy.special import gammaln\n",
    "from jax.numpy import log1p, log, exp\n",
    "\n",
    "\n",
    "def logical_or_three(x1, x2, x3):\n",
    "    return jnp.logical_or(jnp.logical_or(x1, x2), x3)\n",
    "\n",
    "def logical_and_three(x1, x2, x3):\n",
    "    return jnp.logical_and(jnp.logical_and(x1, x2), x3)\n",
    "\n",
    "\n",
    "@custom_jvp\n",
    "def _betainc(a,b,x):\n",
    "    f = jax.scipy.special.betainc\n",
    "    return f(a,b,x) # assign my own\n",
    "\n",
    "# betainc = jax.vmap(_betainc,2)\n",
    "\n",
    "def _betainc_dda_while(args):\n",
    "    summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, digamma_a, x = args\n",
    "    \n",
    "    sum_numer = sum_numer + (digamma_ab - digamma_a) * summand\n",
    "    sum_denom += summand\n",
    "    summand = summand*(1 + (a_plus_b) / k) * (1 + k) / (1 + a_plus_1 / k)\n",
    "    digamma_ab += 1./(a_plus_b + k)\n",
    "    digamma_a += 1./(a_plus_1 + k)\n",
    "    k += 1\n",
    "    summand = summand * (x / k)\n",
    "    # summand = summand*1./10.\n",
    "    args = (summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, digamma_a, x) \n",
    "    \n",
    "    return args\n",
    "\n",
    "\n",
    "def betainc_dda(a_dot, primal_out, a, b, x):\n",
    "    out = jnp.where(\n",
    "        jnp.logical_or(\n",
    "            jnp.logical_or(\n",
    "                jnp.logical_or(\n",
    "                    jnp.logical_and(x > .75, a < 500),\n",
    "                    jnp.logical_and(x > .9, a < 2500)\n",
    "                ),\n",
    "                jnp.logical_or(\n",
    "                    jnp.logical_and(x > .99, a < 1e5),\n",
    "                    x > .999\n",
    "                )\n",
    "            ),\n",
    "            jnp.logical_and(\n",
    "            b > a,\n",
    "            logical_or_three(\n",
    "                logical_and_three(x > .1, x <= .75, b > 500),\n",
    "                logical_and_three(x > .01, x <= .1, b > 2500),\n",
    "                logical_and_three(x > .001, x <= .01, b > 1e5)   \n",
    "            )\n",
    "        )\n",
    "        ),\n",
    "        -1.*_betainc_ddb(a_dot, primal_out, b, a, 1 - x),\n",
    "        _betainc_dda(a_dot, primal_out, b, a, 1.-x)\n",
    "    )\n",
    "    \n",
    "    return out\n",
    "        ## commented for now, need to implement as jax.lax.cond()\n",
    "#     if b > a:\n",
    "#         if ((0.1 < x and x <= 0.75 and b > 500) or (0.01 < x and x <= 0.1 and b > 2500) or (0.001 < x and x <= 0.01 and b > 1e5)):\n",
    "#             return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "\n",
    "#     elif (x > 0.75 and a < 500):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "#     elif (x > 0.9 and a < 2500):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "#     elif (x > 0.99 and a < 1e5):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "#     elif (x > 0.999):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "\n",
    "\n",
    "\n",
    "## define partial derivative wrt a\n",
    "def _betainc_dda(a_dot, primal_out, a, b, x):\n",
    "    # a = jnp.asarray(a)\n",
    "    # b = jnp.asarray(a)\n",
    "    # x = jnp.asarray(x)\n",
    "    \n",
    "    digamma_a = jax.scipy.special.digamma(a)\n",
    "    digamma_ab = jax.scipy.special.digamma(a+b)\n",
    "    \n",
    "\n",
    "    \n",
    "    threshold = 1e-10\n",
    "\n",
    "    a_plus_b = a + b\n",
    "    a_plus_1 = a + 1.\n",
    "    \n",
    "    digamma_a = digamma_a + 1./a\n",
    "    \n",
    "    prefactor = jnp.power(a_plus_1 / a_plus_b, 3)\n",
    "    sum_numer = (digamma_ab - digamma_a) * prefactor\n",
    "    sum_denom = prefactor\n",
    "    summand = prefactor * x * a_plus_b / a_plus_1\n",
    "    \n",
    "    sum_numer = jnp.full_like(x, sum_numer)\n",
    "    sum_denom = jnp.full_like(x, sum_denom)\n",
    "    summand = jnp.full_like(x, summand)\n",
    "    \n",
    "    k = jnp.array(1)\n",
    "    digamma_ab = digamma_ab + 1./a_plus_b\n",
    "    digamma_a = digamma_a + 1./a_plus_1\n",
    "    \n",
    "    ### ----- 6/9/22, 4:03 PM something in the while loop changing the types?\n",
    "    out = jax.lax.while_loop(\n",
    "        lambda args: jnp.any( jnp.logical_and(jnp.abs(args[0]) > 1e-10, args[4] < 1e5) ),\n",
    "        _betainc_dda_while,\n",
    "        (summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, digamma_a, x)\n",
    "    )\n",
    "    \n",
    "    summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, digamma_a, x = out\n",
    "    \n",
    "#     while jnp.any(jnp.abs(summand) > threshold): # transform this to jax.lax.while_loop() to vectorize ?\n",
    "#         sum_numer += (digamma_ab - digamma_a) * summand\n",
    "#         sum_denom += summand\n",
    "#         summand = summand*(1 + (a_plus_b) / k) * (1 + k) / (1 + a_plus_1 / k)\n",
    "#         digamma_ab += 1./(a_plus_b + k)\n",
    "#         digamma_a += 1./(a_plus_1 + k)\n",
    "#         k += 1\n",
    "#         summand = summand * (x / k)\n",
    "        \n",
    "#         if k > 1e5:\n",
    "#             return 1./0.\n",
    "    \n",
    "    \n",
    "    return _betainc(a, b, x) * (log(x) + sum_numer / sum_denom)*a_dot\n",
    "\n",
    "\n",
    "def _betainc_ddb_while(args):\n",
    "    summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, x = args\n",
    "    \n",
    "    sum_numer += digamma_ab * summand\n",
    "    sum_denom += summand\n",
    "\n",
    "    summand = summand*(1 + (a_plus_b) / k) * (1 + k) / (1 + a_plus_1 / k)\n",
    "    digamma_ab += 1./(a_plus_b + k)\n",
    "    k +=1\n",
    "    summand = summand * x / k\n",
    "    # summand = summand*1./10.\n",
    "    args = (summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, x)\n",
    "    \n",
    "    return args\n",
    "\n",
    "    # jax.lax.cond(jnp.any(k > 1e5),lambda x: 1.0/0., lambda x: pass): # nothing to stop infinite loop for now\n",
    "        \n",
    "\n",
    "def betainc_ddb(b_dot, primal_out, a, b, x):\n",
    "    out = jnp.where(\n",
    "    jnp.logical_and(b > a, \n",
    "    logical_or_three(\n",
    "        logical_and_three(\n",
    "            x > .1, x <= .75, b > 500),\n",
    "        logical_and_three(\n",
    "            x > .01, x <= .1, b > 2500\n",
    "        ),\n",
    "        logical_and_three(\n",
    "            x > .001, x <= .01, b > 1e5\n",
    "        )\n",
    "        )\n",
    "    ),\n",
    "    -1.*_betainc_dda(b_dot, primal_out, b, a, 1.-x),\n",
    "    _betainc_ddb(b_dot, primal_out, a, b, x)\n",
    "    )\n",
    "        \n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## define partial derivative wrt b\n",
    "def _betainc_ddb(b_dot, primal_out, a, b, x):\n",
    "    \n",
    "    digamma_b = jax.scipy.special.digamma(b)\n",
    "    digamma_ab = jax.scipy.special.digamma(a+b)\n",
    "\n",
    "    ## commented for now, need to implement as jax.lax.cond()\n",
    "    \n",
    "\n",
    "\n",
    "#     if (b > a):\n",
    "#         if ((0.1 < x and x <= 0.75 and b > 500) or (0.01 < x and x <= 0.1 and b > 2500) or (0.001 < x and x <= 0.01 and b > 1e5)):\n",
    "#             return -_betainc_dda(b_dot, primal_out, b, a, 1 - x);\n",
    "\n",
    "\n",
    "#     if ((x > 0.75 and a < 500) or (x > 0.9 and a < 2500) or (x > 0.99 and a < 1e5) or (x > 0.999)):\n",
    "#         return -_betainc_dda(b_dot, primal_out, b, a, 1 - x);\n",
    "    \n",
    "    threshold = 1e-10\n",
    "    \n",
    "    a_plus_b = a + b\n",
    "    a_plus_1 = a + 1.\n",
    "    \n",
    "    prefactor = jnp.power(a_plus_1 / a_plus_b, 3)\n",
    "    \n",
    "    sum_numer = digamma_ab * prefactor\n",
    "    sum_denom = prefactor\n",
    "    summand = prefactor * x * a_plus_b / a_plus_1\n",
    "    \n",
    "    sum_numer = jnp.full_like(x, sum_numer)\n",
    "    sum_denom = jnp.full_like(x, sum_denom)\n",
    "    summand = jnp.full_like(x, summand)\n",
    "    \n",
    "    k = jnp.array(1)\n",
    "    digamma_ab = digamma_ab + 1./a_plus_b\n",
    "    \n",
    "    out = jax.lax.while_loop(\n",
    "        lambda args: jnp.any( jnp.logical_and(jnp.abs(args[0]) > 1e-10, args[4] < 1e5) ),\n",
    "        _betainc_ddb_while,\n",
    "        (summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, x)\n",
    "    )\n",
    "    \n",
    "    summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, x = out\n",
    "    \n",
    "#     while jnp.any(jnp.abs(summand) > threshold): # transform this to jax.lax.while_loop() to vectorize ?\n",
    "#         sum_numer += digamma_ab * summand\n",
    "#         sum_denom += summand\n",
    "\n",
    "#         summand = summand*(1 + (a_plus_b) / k) * (1 + k) / (1 + a_plus_1 / k)\n",
    "#         digamma_ab += 1./(a_plus_b + k)\n",
    "#         k +=1\n",
    "#         summand = summand * x / k\n",
    "        \n",
    "#         if k > 1e5:\n",
    "#             return 1./0.        \n",
    "        \n",
    "    \n",
    "    return _betainc(a, b, x) * (log(1 - x) - digamma_b + sum_numer / sum_denom)*b_dot\n",
    "\n",
    "\n",
    "def betainc_gradx(g, primal_out, a, b, x):\n",
    "    lbeta = gammaln(a) + gammaln(b) - gammaln(a + b)\n",
    "    partial_x = exp((b - 1) * log1p(-x) +\n",
    "                  (a - 1) * log(x) - lbeta)\n",
    "    return partial_x * g\n",
    "\n",
    "\n",
    "_betainc.defjvps(betainc_dda, betainc_ddb, betainc_gradx)\n",
    "\n",
    "# betainc_dda(jnp.array([1.,1.,1.]), None, jnp.array([3., 5, 1]), jnp.array([3., 5, 1]), jnp.array([.5,.6, .9]))\n",
    "\n",
    "# # print(grad(_betainc)(2., 3., .5))\n",
    "print(grad(_betainc, 0)(2., 1., .6))  # same as above\n",
    "print(grad(_betainc, 1)(2., 1., .6))\n",
    "print(grad(_betainc, 2)(2., 1., .6))\n",
    "\n",
    "# _betainc.defjvps(None, None, betainc_gradx)\n",
    "# print( grad(_betainc,2)(jnp.array([2.,3]), jnp.array([2.,3]), jnp.array([.5,.7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "round-place",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.24999976, 0.875     ], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from jax.scipy.special import betainc, betaln, gammaln\n",
    "\n",
    "# grad(_betainc, 0)(jnp.array([3., 5, 1]), jnp.array([3., 5, 1]), jnp.array([.5,.6, .9]))\n",
    "\n",
    "_betainc(np.array([1,1]), np.array([1,3]), np.array([.25, .5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "short-tobacco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting betaincder\n",
      "  Downloading betaincder-0.1.1.tar.gz (27 kB)\n",
      "Building wheels for collected packages: betaincder\n",
      "  Building wheel for betaincder (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for betaincder: filename=betaincder-0.1.1-cp38-cp38-linux_x86_64.whl size=81494 sha256=74622bd42053b7c2c36ed92bb3078d87e2712db7b49d4c49003e345807ac1926\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/a5/2d/eb/7ddc4f8ae93ba762e10d64dbc36768922dd77587a1fe7a56bd\n",
      "Successfully built betaincder\n",
      "Installing collected packages: betaincder\n",
      "Successfully installed betaincder-0.1.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install betaincder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "rough-estate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from betaincder import betaincderp, betaincderq\n",
    "\n",
    "vec_betaincderp = np.vectorize(betaincderp)\n",
    "vec_betaincderq = np.vectorize(betaincderq)\n",
    "# np.vectorize(betaincderp)(jnp.array([.5, .5]), jnp.array([10., 20.]), jnp.array([10.,10]))\n",
    "\n",
    "@custom_jvp\n",
    "def _betainc(a,b,x):\n",
    "    f = jax.scipy.special.betainc\n",
    "    return f(a,b,x) # assign my own\n",
    "\n",
    "def betainc_grada(g, primal_out, a, b, x):\n",
    "    return g*vec_betaincderp(x, a, b)\n",
    "\n",
    "def betainc_gradb(g, primal_out, a, b, x):\n",
    "    return g*vec_betaincderq(x, a, b)\n",
    "\n",
    "\n",
    "def betainc_gradx(g, primal_out, a, b, x):\n",
    "    lbeta = gammaln(a) + gammaln(b) - gammaln(a + b)\n",
    "    partial_x = exp((b - 1) * log1p(-x) +\n",
    "                  (a - 1) * log(x) - lbeta)\n",
    "    return partial_x * g\n",
    "\n",
    "_betainc.defjvps(betainc_grada, betainc_gradb, betainc_gradx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "focused-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.18389723\n",
      "0.34642607\n",
      "1.1999998\n"
     ]
    }
   ],
   "source": [
    "# _betainc(np.array([1,1]), np.array([1,3]), np.array([.25, .5]))\n",
    "print(grad(_betainc, 0)(2., 1., .6))  # same as above\n",
    "print(grad(_betainc, 1)(2., 1., .6))\n",
    "print(grad(_betainc, 2)(2., 1., .6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "silent-carry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09109296, -0.01093529])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "stuck-pennsylvania",
   "metadata": {},
   "source": [
    "# Testing on some rounded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "digital-arrangement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([ 7,  9,  9,  8,  6,  7,  8,  9,  9,  8,  7,  6,  8,  9,  7,\n",
       "              8,  8,  8,  8,  9,  6, 10,  6,  9,  5,  8,  9, 10,  7,  8,\n",
       "              9,  7,  7,  8,  6,  6,  8,  8,  5,  8,  7,  4,  9,  9,  9,\n",
       "              7,  9,  9,  8,  8,  6,  6,  8,  5,  6,  8,  9,  8,  4,  8,\n",
       "              7,  6,  4,  7,  8,  7,  7,  7,  8,  8,  6,  9,  9,  8,  7,\n",
       "              5,  9,  9,  6,  9,  9,  7,  8,  5,  5,  7,  9,  6,  7,  8,\n",
       "              8,  9,  8,  5,  8,  7,  8,  7,  7,  7], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.random import PRNGKey\n",
    "\n",
    "X_raw = dist.Beta(.75*10, (1-.75)*10).sample(PRNGKey(10), (100,))\n",
    "x_round = round(X_raw*10)\n",
    "x_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "collaborative-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mymodel(x=None):\n",
    "    mu = numpyro.sample(\"mu\", dist.Beta(1,1)) # noise parameter\n",
    "    k = numpyro.sample(\"k\", dist.HalfCauchy(10)) # noise parameter\n",
    "    \n",
    "    with numpyro.plate(\"data\", x.shape[0]):\n",
    "        xhat = numpyro.sample(\"xhat\", dist.Beta(mu*k, (1-mu)*k), obs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "senior-fisher",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 4000/4000 [00:03<00:00, 1075.68it/s, 3 steps of size 8.10e-01. acc. prob=0.92]\n"
     ]
    }
   ],
   "source": [
    "kernel = NUTS(mymodel, target_accept_prob=.80)\n",
    "\n",
    "mcmc = MCMC(kernel, \n",
    "               num_warmup=2_000, \n",
    "               num_samples=2_000, \n",
    "               num_chains=1)\n",
    "\n",
    "mcmc.run(random.PRNGKey(0), X_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "imported-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "         k      8.81      1.19      8.77      6.87     10.73   1224.80      1.00\n",
      "        mu      0.75      0.01      0.75      0.72      0.77   1217.59      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dimensional-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def betacdf(alpha, beta, x):\n",
    "#     vals = jnp.arange(1000+1)\n",
    "#     probs = jnp.exp(dist.BetaBinomial(alpha, beta, 1000).log_prob(vals))\n",
    "#     # then sum up probs below x \n",
    "#     validprobs = jnp.where(vals < x*1000, probs, 0.)\n",
    "    \n",
    "#     return jnp.sum(validprobs)\n",
    "\n",
    "# betacdf = jnp.vectorize(betacdf)\n",
    "\n",
    "# betacdf(3, 3, .5)\n",
    "\n",
    "# def betacdf(alpha, beta, x):\n",
    "#     vals = jnp.linspace(1e-5, 1.-1e-5, num=10000)\n",
    "#     probs = jnp.exp(dist.Beta(alpha, beta).log_prob(vals))\n",
    "#     # then sum up probs below x \n",
    "#     validprobs = jnp.where(vals <= x, probs, 0.)\n",
    "    \n",
    "#     return jnp.sum(validprobs/10000.)\n",
    "\n",
    "# betacdf = jnp.vectorize(betacdf)\n",
    "\n",
    "# betacdf(1., 1., .500)\n",
    "\n",
    "# this very naive approximation seems to work? but how/why when it didn't seem to work before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "parallel-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability.substrates.jax as tfp\n",
    "\n",
    "def f(mu, k):\n",
    "    a = mu*k\n",
    "    b = (1.-mu)*k\n",
    "    \n",
    "    responses = jnp.linspace(0,10, num=11)\n",
    "    lower = jnp.clip((responses/10.) - .05, 1e-3, 1-1e-3)\n",
    "    upper = jnp.clip((responses/10.) + .05, 1e-3, 1-1e-3)\n",
    "    \n",
    "    prob_resps = tfp.math.betainc(a, b, upper) - tfp.math.betainc(a, b, lower)\n",
    "    prob_resps = prob_resps / jnp.sum(prob_resps)\n",
    "    \n",
    "    return(prob_resps)\n",
    "\n",
    "\n",
    "def mymodel_round(x=None):\n",
    "    mu = numpyro.sample(\"mu\", dist.Beta(1,1)) # noise parameter\n",
    "    k = numpyro.sample(\"k\", dist.HalfCauchy(10)) # noise parameter\n",
    "\n",
    "    resp_probs = f(mu,k)\n",
    "    resp_probs = resp_probs/jnp.sum(resp_probs)\n",
    "    \n",
    "    with numpyro.plate(\"data\", x.shape[0]):\n",
    "\n",
    "        xhat = numpyro.sample(\"xhat\", dist.Categorical(probs=resp_probs), obs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "cardiac-removal",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 2000/2000 [00:08<00:00, 243.85it/s, 3 steps of size 8.52e-01. acc. prob=0.91] \n"
     ]
    }
   ],
   "source": [
    "from numpyro.infer import init_to_median, init_to_value\n",
    "\n",
    "kernel = NUTS(mymodel_round, target_accept_prob=.80, init_strategy = init_to_value(values={\"mu\":.33, \"k\":5}))\n",
    "\n",
    "mcmc = MCMC(kernel, \n",
    "               num_warmup=1_000, \n",
    "               num_samples=1_000, \n",
    "               num_chains=1)\n",
    "\n",
    "mcmc.run(random.PRNGKey(0), x_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "modified-residence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "         k      9.69      1.43      9.69      7.50     12.03    785.03      1.00\n",
      "        mu      0.75      0.01      0.75      0.73      0.77    776.91      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "independent-nevada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ggplot: (8791051983758)>"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = dist.Normal(0.,1.).sample(PRNGKey(1), (100,))\n",
    "y = expit(-1 + 1.5*x_norm)\n",
    "y = dist.Beta(y*50., (1-y)*50.).sample(PRNGKey(1))\n",
    "\n",
    "y_round_10 = round(y*10)\n",
    "y_round_5 = round(y*20)\n",
    "\n",
    "\n",
    "# x_binary = dist.Bernoulli(.5).sample(PRNGKey(1), (100,))\n",
    "# y = .25 + x_binary*.50\n",
    "# y = dist.Beta(y*50., (1-y)*50.).sample(PRNGKey(0))\n",
    "\n",
    "# y_round_bin10 = round(y*10)\n",
    "# y_round\n",
    "\n",
    "ggplot(aes(x=x_norm, y=y_round_10)) + geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "detected-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jax import vmap\n",
    "# vmap(f,(0,0))(jnp.array([.11, .21, .3]), jnp.array([10, 20, 30]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "devoted-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(mu, k):\n",
    "    a = mu*k\n",
    "    b = (1.-mu)*k\n",
    "    \n",
    "    rnd_unit_scaled = 1/10.\n",
    "    \n",
    "#     responses = jnp.linspace(1, 9, num=9)\n",
    "#     cuts = responses/10.\n",
    "\n",
    "#     upper_probs = jnp.pad(tfp.math.betainc(a, b, cuts+.05), (0,1), constant_values=(0.,1.))\n",
    "#     lower_probs = jnp.pad(tfp.math.betainc(a, b, cuts-.05), (1,0), constant_values=(0.,1.))\n",
    "\n",
    "    responses = jnp.linspace(0, 10, num=11)\n",
    "    cuts = responses/10.\n",
    "\n",
    "    upper_probs = jnp.pad(tfp.math.betainc(a, b, cuts[0:10]+.05), (0,1), constant_values=(0.,1.))\n",
    "    lower_probs = jnp.pad(tfp.math.betainc(a, b, cuts[1:11]-.05), (1,0), constant_values=(0.,1.))\n",
    "\n",
    "    prob_resps = upper_probs - lower_probs\n",
    "    prob_resps = (prob_resps + 1e-16) / jnp.sum(prob_resps)\n",
    "    \n",
    "    return(prob_resps)\n",
    "\n",
    "\n",
    "responses_10 = jnp.linspace(0, 10, num=11)\n",
    "responses_5 = jnp.linspace(0, 20, num=21)\n",
    "responses_1 = jnp.linspace(0, 101, num=101)\n",
    "\n",
    "# def f2(mu, k, responses):\n",
    "#     a = mu*k\n",
    "#     b = (1.-mu)*k\n",
    "#     n_resps = (responses.shape[0]-1)\n",
    "#     rnd_unit_scaled = 1/n_resps\n",
    "    \n",
    "#     # responses = jnp.linspace(0, 10, num=11)\n",
    "#     lower = jnp.clip((responses/n_resps) - rnd_unit_scaled/2., 1e-3, 1-1e-3)\n",
    "#     upper = jnp.clip((responses/n_resps) + rnd_unit_scaled/2., 1e-3, 1-1e-3)\n",
    "    \n",
    "#     prob_resps = tfp.math.betainc(a, b, upper) - tfp.math.betainc(a, b, lower)\n",
    "#     prob_resps = (prob_resps + 1e-16) / jnp.sum(prob_resps) # add err to prevent divergences\n",
    "    \n",
    "#     return(prob_resps)\n",
    "\n",
    "\n",
    "f2_multi = vmap(f2, (0, None))\n",
    "\n",
    "def mymodel_round2(x, y=None):\n",
    "    \n",
    "    alpha = numpyro.sample(\"alpha\", dist.Normal(0,1))\n",
    "    beta = numpyro.sample(\"beta\", dist.Normal(0,1))\n",
    "    k = numpyro.sample(\"k\", dist.HalfCauchy(10)) # noise parameter\n",
    "    \n",
    "    mu = expit(alpha + beta*x)\n",
    "\n",
    "    resp_probs = f2_multi(mu,k)\n",
    "    \n",
    "    with numpyro.plate(\"data\", x.shape[0]):\n",
    "\n",
    "        yhat = numpyro.sample(\"yhat\", dist.Categorical(probs=resp_probs), obs=y) # rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-delight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample:  71%|███████   | 1423/2000 [01:30<00:36, 15.93it/s, 3 steps of size 7.03e-01. acc. prob=0.92] "
     ]
    }
   ],
   "source": [
    "kernel2 = NUTS(\n",
    "    mymodel_round2, \n",
    "    target_accept_prob=.80#,\n",
    "    # init_strategy = init_to_value(values = {\"alpha\":0., \"beta\":0., \"k\":5.})\n",
    ")\n",
    "\n",
    "mcmc2 = MCMC(kernel2, \n",
    "               num_warmup=1_000, \n",
    "               num_samples=1_000, \n",
    "               num_chains=1)\n",
    "\n",
    "mcmc2.run(random.PRNGKey(0), x_norm, y_round_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "crucial-jurisdiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "     alpha     -1.04      0.04     -1.04     -1.11     -0.97    799.47      1.00\n",
      "      beta      1.49      0.05      1.48      1.40      1.58    858.86      1.00\n",
      "         k     50.11      9.74     48.75     34.80     65.23    847.19      1.00\n",
      "\n",
      "Number of divergences: 0\n"
     ]
    }
   ],
   "source": [
    "mcmc2.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-combine",
   "metadata": {},
   "source": [
    "## it works!????\n",
    "\n",
    "seems to work with the betabinomial approximation above!??!?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "homeless-melissa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([0.5       , 0.18000002], dtype=float32),\n",
       " DeviceArray([0.        , 0.12600006], dtype=float32),\n",
       " DeviceArray([1.        , 0.21600002], dtype=float32),\n",
       " DeviceArray([2., 5.], dtype=float32),\n",
       " 1,\n",
       " DeviceArray([2., 3.], dtype=float32),\n",
       " DeviceArray([0.9227846, 1.7061182], dtype=float32),\n",
       " DeviceArray([0.9227846, 1.2561179], dtype=float32),\n",
       " DeviceArray([0.5, 0.5], dtype=float32))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = jnp.array([.5,.5])\n",
    "a, b = jnp.array([1.,2.]), jnp.array([1.,3.])\n",
    "# x = jnp.array([.5])\n",
    "# a, b = 1., 1.\n",
    "\n",
    "digamma_a = jax.scipy.special.digamma(a)\n",
    "digamma_ab = jax.scipy.special.digamma(a+b)\n",
    "\n",
    "a_plus_b = a + b\n",
    "a_plus_1 = a + 1\n",
    "\n",
    "digamma_a = digamma_a + 1./a\n",
    "\n",
    "prefactor = jnp.power(a_plus_1 / a_plus_b, 3)\n",
    "sum_numer = (digamma_ab - digamma_a) * prefactor\n",
    "sum_denom = prefactor\n",
    "summand = prefactor * x * a_plus_b / a_plus_1\n",
    "\n",
    "k = 1\n",
    "digamma_ab = digamma_ab + 1./a_plus_b\n",
    "digamma_a = digamma_a + 1./a_plus_1\n",
    "\n",
    "inits = (summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, digamma_a, x)\n",
    "\n",
    "inits\n",
    "# _betainc_dda_while(inits)\n",
    "\n",
    "# out = jax.lax.while_loop(\n",
    "#         lambda args: jnp.any(jnp.abs(args[0]) > 1e-10),\n",
    "#         _betainc_dda_while,\n",
    "#         inits\n",
    "#     )\n",
    "\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "documentary-missile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "## define partial derivative wrt a\n",
    "def _betainc_dda2(a_dot, primal_out, a, b, x):\n",
    "    # a = jnp.asarray(a)\n",
    "    # b = jnp.asarray(a)\n",
    "    # x = jnp.asarray(x)\n",
    "    \n",
    "    digamma_a = jax.scipy.special.digamma(a)\n",
    "    digamma_ab = jax.scipy.special.digamma(a+b)\n",
    "    \n",
    "    ## commented for now, need to implement as jax.lax.cond()\n",
    "#     if b > a:\n",
    "#         if ((0.1 < x and x <= 0.75 and b > 500) or (0.01 < x and x <= 0.1 and b > 2500) or (0.001 < x and x <= 0.01 and b > 1e5)):\n",
    "#             return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "\n",
    "#     elif (x > 0.75 and a < 500):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "#     elif (x > 0.9 and a < 2500):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "#     elif (x > 0.99 and a < 1e5):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "#     elif (x > 0.999):\n",
    "#         return -_betainc_ddb(a_dot, primal_out, b, a, 1 - x)\n",
    "    \n",
    "    threshold = 1e-10\n",
    "\n",
    "    a_plus_b = a + b\n",
    "    a_plus_1 = a + 1.\n",
    "    \n",
    "    digamma_a = digamma_a + 1./a\n",
    "    \n",
    "    prefactor = jnp.power(a_plus_1 / a_plus_b, 3)\n",
    "    sum_numer = (digamma_ab - digamma_a) * prefactor\n",
    "    sum_denom = prefactor\n",
    "    summand = prefactor * x * a_plus_b / a_plus_1\n",
    "    \n",
    "    k = jnp.array(1)\n",
    "    digamma_ab = digamma_ab + 1./a_plus_b\n",
    "    digamma_a = digamma_a + 1./a_plus_1\n",
    "    \n",
    "    inits = (summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, digamma_a, x)\n",
    "    ### ----- 6/9/22, 4:03 PM something in the while loop changing the types?\n",
    "    out = jax.lax.while_loop(\n",
    "        lambda args: jnp.any(jnp.abs(args[0]) > 1e-10),\n",
    "        _betainc_dda_while,\n",
    "        inits\n",
    "    )\n",
    "    \n",
    "    summand, sum_numer, sum_denom, a_plus_b, k, a_plus_1, digamma_ab, digamma_a, x = out\n",
    "    \n",
    "#     while jnp.any(jnp.abs(summand) > threshold): # transform this to jax.lax.while_loop() to vectorize ?\n",
    "#         sum_numer += (digamma_ab - digamma_a) * summand\n",
    "#         sum_denom += summand\n",
    "#         summand = summand*(1 + (a_plus_b) / k) * (1 + k) / (1 + a_plus_1 / k)\n",
    "#         digamma_ab += 1./(a_plus_b + k)\n",
    "#         digamma_a += 1./(a_plus_1 + k)\n",
    "#         k += 1\n",
    "#         summand = summand * (x / k)\n",
    "        \n",
    "#         if k > 1e5:\n",
    "#             return 1./0.\n",
    "    \n",
    "    \n",
    "    return (inits, out)\n",
    "\n",
    "\n",
    "z, w = _betainc_dda2(jnp.array([1.,1.]), None, jnp.array([1.,3.]), jnp.array([1.,3.]), jnp.array([.1,.3]))\n",
    "print(z[3].shape)\n",
    "print(w[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "selective-reliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(3.5303732e-11, dtype=float32, weak_type=True),\n",
       " DeviceArray(0.20089278, dtype=float32, weak_type=True),\n",
       " DeviceArray(0.68877554, dtype=float32, weak_type=True),\n",
       " DeviceArray(4., dtype=float32, weak_type=True),\n",
       " DeviceArray(21, dtype=int32, weak_type=True),\n",
       " DeviceArray(3., dtype=float32, weak_type=True),\n",
       " DeviceArray(3.1987426, dtype=float32, weak_type=True),\n",
       " DeviceArray(3.157076, dtype=float32, weak_type=True),\n",
       " DeviceArray(0.3, dtype=float32, weak_type=True))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, w = _betainc_dda2(jnp.array(1.), None, jnp.array(2.), jnp.array(2.), jnp.array(.3))\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "cellular-effects",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([6], dtype=int32)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z+=1\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-spelling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
